"""
    Tune reranker 
"""
import argparse
import json
import os
import random
import torch
import logging
import copy
import wandb

from itertools import product
from collections import defaultdict
import numpy as np

from transformers import AutoTokenizer, BartConfig

from metax.models.mybart import MyBart
from metax.task_manager import dataloader
from metax.task_manager.dataloader import GeneralDataset
from metax.reranker_bootstrap.utils import * 
from metax.utils import loss_evaluate, model_train, get_optimizer
from metax.models.utils import trim_batch

from simpletransformers.classification import ClassificationModel, ClassificationArgs


# ================ Global Vars ================
# This will be updated by argparse, no need to manually update
MODEL_TYPE = None
main_logger = init_logger()
# =============== Helper Functions =================

def log(msg):
    main_logger.info(msg)


def log_args(args):
    """
        Log initial arguments
    """
    for k, v in vars(args).items():
        main_logger.log(logging.INFO, f"{k:>30} ====> {v}")



def load_reranker(path, model_type):
    """
        Load reranker checkpoint
    """
    log(f"Loading Reranker from {path}...")
    model = ClassificationModel(
        model_type, path
    )
    log("Reranker Loaded")
    return model

def create_reranker(output_dir, model_type, model_name):
    model_args = ClassificationArgs(num_train_epochs=5, 
                                use_hf_datasets=True,
                                no_cache=False,
                                do_lower_case=True,
                                fp16=True,
                                overwrite_output_dir=True,
                                train_batch_size=64, 
                                eval_batch_size=64, 
                                output_dir=output_dir,
                                max_seq_length=512,
                                learning_rate=1e-5,
                                warmup_steps=100,
                                n_gpu=torch.cuda.device_count(),
                                evaluate_during_training_verbose=True,
                                evaluate_during_training_steps=200,
                                save_steps=400,
                                wandb_project="recross",
                                evaluate_during_training=True)
    model = ClassificationModel(
        model_type, model_name, args=model_args, num_labels=2
    )

    return model

def fine_tune(reranker, train_df, dev_df, output_path):
    arg_updates = {
        "output_dir" : output_path,
        "num_train_epochs" : 3,
        "n_gpu" : torch.cuda.device_count(),
        "learning_rate" : 5e-5  
    }
    reranker.train_model(train_df, eval_df = dev_df, args=arg_updates)

def init_dirs(args):
    make_dir(args.save_path)
    make_dir(args.data_backup_path)


def get_parser():
    parser = argparse.ArgumentParser(
        description="Bootstrap Reranker")

    parser.add_argument('--initial_checkpoint_path', type=str,
                        help="Path to R_i checkpoint in bootstrapping. If nothing means first iteration.")
    
    parser.add_argument('--train_tuple_path', type=str, required=True,
                    help="Path to the merged json file generated by gen_better_ds.py for training.")
    parser.add_argument('--dev_tuple_path', type=str, 
                help="Path to the merged json file generated by gen_better_ds.py for dev.")

    parser.add_argument('--checkpoint_save_path', default="yuchenlin/BART0pp",
                        type=str, help="Path to save R_(i+1) checkpoint in bootstrapping")
    parser.add_argument('--reranker_model_type', required=True,
                        type=str, help="Type of reranker model (e.g. bart)")
    parser.add_argument('--reranker_model_name', required=True,
                        type=str, help="Name of reranker model (e.g. facebook/bart-base)")

    return parser


def main():
    args = get_parser().parse_args()
    log_args(args)

    # Create output directories
    make_dir(args.checkpoint_save_path)

    wandb.init(project="recross", entity="yuchenlin",
               settings=wandb.Settings(start_method="fork"))

    # There is no reranker yet in the first iteration
    is_first_iteration = True if not args.initial_checkpoint_path else False

    if is_first_iteration:
        log("No previous checkpoint specified, train reranker from scratch.")
        reranker = create_reranker(args.checkpoint_save_path, args.reranker_model_type, args.reranker_model_name)
    else:
        reranker = load_reranker(args.initial_checkpoint_path, args.reranker_model_type)

    train_pairs = assemble_pairs(load_data_from_json(args.train_tuple_path),mode="product")
    
    dev_pairs = assemble_pairs(load_data_from_json(args.dev_tuple_path),mode="product")

    log(f"Size of training data {len(train_pairs)}")
    log(f"Size of dev data {len(dev_pairs)}")

    train_df = create_model_dataframe(train_pairs)
    dev_df = create_model_dataframe(dev_pairs)

    log(f"Start fine-tuning reranker using better ds data.")

    # Here we turn off evalutate during training to save all data for training
    log(f"Will save tuned reranker to {args.checkpoint_save_path}")
    # Seems like setting output_dir parameter cannot let the model save

    if is_first_iteration:
        reranker.train_model(train_df, eval_df = dev_df)
    else:
        fine_tune(reranker, train_df, dev_df, args.checkpoint_save_path)

if __name__ == "__main__":
    main()
