# How to run bootstrapping

## To do one bootstrap step, you need
1. Use `gen_bart_query.sh` to generate a certain number of queries. This script does nothing but select some queries from ds data generated by `ds_gen.py`. It makes sure that all groups have equal opportunities of being in the query set. Notice we are currently using training split of ds data, a.k.a. `ds_from_bart0_upstream_train.json`. You can specify how many queries you want to select by specifying `--N`. The queries will be stored in `data/bootstrap_reranker/bart_queries/` in a single json file. It's a list of queries (i.e. a list of list of formatted instances).

2. Use `retrieve_with_bart.sh` to use bart retirever to retrieve instances according to the queries you just generated in part 1. This file will generate a list of tuples. Each tuple has two keys `query_examples` and `retrieved_candidates`. Normally `query_examples` contains 8 instances and `retrieved_candidates` contains 100+ instances. But you can configure these in the script arg. This script will also make the final list into several shards to facilitate parallelism in the next step. The sharded list will be stored in `data/bootstrap_reranker/bart_retrieved/`

3. Use `gen_better_ds_all.sh` to generate better distant supervision data for bootstrapping. This script will submit multiple slurm jobs, each running a shard from previous step. So if you want to have n slurm jobs running in parallel, you should shard the data to n pieces in the previous step. For each shard containing k (query, candidate) pairs, this step will generate k (query, positive, negative) distant supervision data. 

4. Use `merge_tuples.sh` to merge better ds data generated by each job in the previous step. 

5. Use `tune_reranker.sh` to train the reranker.

---
## Example usage

1. `cd MetaCross/`
2. `./scripts/reranker_bootstrap/gen_bart_query.sh 300` Select 300 queries from ds data.
3. `sbatch ./scripts/reranker_bootstrap/retrieve_with_bart.sh` to retrieve with bart. Make sure to update the `--query_path` parameter to be the file you generated from the previous step. Also, you might want to adjust `--shards`. This will determine how many jobs you want to run in parallel in the next step. For example, if you have 6 shards, the next step will be able to divide the work load into 6 slurm jobs each using 1 GPU. 
4. `./scripts/reranker_bootstrap/gen_better_ds_all.sh` to start generating better distant supervision data. Don't forget to adjust `SHARDS` in this script to be the same as the value of `--shards` in the previous step. You also have to adjust `--initial_checkpoint_path` in `gen_better_ds_one.sh`. If this is your first bootstrap iteration, don't pass in this argument and the script will simply take topK in sampled_candidates first without reranking. If this is the i-th bootstrapping iteration, pass in the path of the checkpoint of the previous iteration. 
5. `./scripts/reranker_bootstrap/merge_tuples.sh` to merge the results of different jobs from the previous step. Make sure to adjust `--data_save_path` to be parent folder where you stored the results of all the previous steps. i.e. it should be the folder that contains a bunch of folders `better_ds_i_outof_n/`. 
6. `sbatch ./scripts/reranker_bootstrap/tune_reranker.sh` to train reranker. Make sure to set `--tuple_file_path` as the data generated in the previous step, and `--checkpoint_save_path` as where you want to save the tuned model. Similar to step 4, if this is your first bootstrap iteration, don't pass in `--initial_checkpoint_path` and the reranker will be trained from scratch. 